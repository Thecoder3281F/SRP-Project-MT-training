{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dad385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Thecoder3281f/MIT_separated\", \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c06113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"t5-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacde78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"][0][\"attention_mask\"]  # Example of tokenized input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46fab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def canonicalize(smiles):\n",
    "    \"\"\"Join tokens, parse to molecule, return canonical SMILES or None.\"\"\"\n",
    "    try:\n",
    "        s = smiles.replace(\" \", \"\")\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def tanimoto(a, b):\n",
    "    \"\"\"Compute Tanimoto similarity between two SMILES.\"\"\"\n",
    "    try:\n",
    "        ma, mb = Chem.MolFromSmiles(a), Chem.MolFromSmiles(b)\n",
    "        if not ma or not mb:\n",
    "            return 0\n",
    "        fa = AllChem.GetMorganFingerprintAsBitVect(ma, 2)\n",
    "        fb = AllChem.GetMorganFingerprintAsBitVect(mb, 2)\n",
    "        return DataStructs.TanimotoSimilarity(fa, fb)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # handle tuple\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # convert logits to token IDs if needed\n",
    "    preds = np.array(preds)\n",
    "    if preds.ndim == 3:  # (batch, seq_len, vocab_size)\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # ðŸ§  handle both top-1 and top-k outputs\n",
    "    if preds.ndim == 3:  # (batch, k, seq_len)\n",
    "        batch_size, k, seq_len = preds.shape\n",
    "        preds = preds.reshape(batch_size * k, seq_len)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_preds = [decoded_preds[i * k:(i + 1) * k] for i in range(batch_size)]\n",
    "    else:  # (batch, seq_len)\n",
    "        decoded_preds = [[tokenizer.decode(p, skip_special_tokens=True)] for p in preds]\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    top1_correct = 0\n",
    "    tanimotos = []\n",
    "    valids = []\n",
    "\n",
    "    for k_preds, label in zip(decoded_preds, decoded_labels):\n",
    "        label_c = canonicalize(label)\n",
    "        \n",
    "\n",
    "        if label_c is None:\n",
    "            tanimotos.append(0)\n",
    "            valids.append(0)\n",
    "            continue\n",
    "        valids.append(bool(Chem.MolFromSmiles(label_c)))\n",
    "        best_tani = 0\n",
    "\n",
    "        for i, pred in enumerate(k_preds):\n",
    "            p_c = canonicalize(pred)\n",
    "            if p_c is None:\n",
    "                continue\n",
    "\n",
    "            tani = tanimoto(p_c, label_c)\n",
    "            best_tani = max(best_tani, tani)\n",
    "\n",
    "            if p_c == label_c:\n",
    "                if i == 0:\n",
    "                    top1_correct += 1\n",
    "                break\n",
    "\n",
    "        tanimotos.append(best_tani)\n",
    "\n",
    "    canonical_top1 = top1_correct / len(decoded_labels)\n",
    "    mean_tanimoto = sum(tanimotos) / len(tanimotos)\n",
    "    validity = sum(valids) / len(valids)\n",
    "\n",
    "    logger.info(f\"Canonical Top-1 Accuracy: {canonical_top1:.3f}\")\n",
    "    logger.info(f\"Mean Tanimoto: {mean_tanimoto:.3f}\")\n",
    "    logger.info(f\"Validity: {validity:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"canonical_top1\": canonical_top1,\n",
    "        \"mean_tanimoto\": mean_tanimoto,\n",
    "        \"validity\": validity,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_small = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(40000))\n",
    "val_dataset_small = tokenized_datasets[\"val\"].shuffle(seed=42).select(range(3000))\n",
    "test_dataset_small = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(3000))\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "val_dataset = tokenized_datasets[\"val\"]\n",
    "test_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1), labels\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test-large\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    # per_device_train_batch_size=64,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=1000,\n",
    "    save_steps=250,\n",
    "    eval_steps=250,\n",
    "    # num_train_epochs=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.05,\n",
    "    logging_dir=\"./logs\",\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"canonical_top1\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset_small,\n",
    "    eval_dataset=val_dataset_small,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "\n",
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     direction=\"minimize\",\n",
    "#     backend=\"optuna\",\n",
    "#     n_trials=5,\n",
    "#     hp_space=lambda trial: {\n",
    "#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4),\n",
    "#         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ac42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(test_dataset_small)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"t5-mit-small-dataset-separated-lr1e-5-wd0.05-5000steps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srp-mt-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
