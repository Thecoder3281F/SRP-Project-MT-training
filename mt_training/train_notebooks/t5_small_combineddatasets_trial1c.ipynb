{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_separated = load_dataset(\"Thecoder3281f/MIT_separated_final\", \"normal\")\n",
    "dataset_mixed = load_dataset(\"Thecoder3281f/MIT_mixed_final\", \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ec7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16e850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # inputs = [x.replace(\" \", \"\") for x in batch[\"input\"]]\n",
    "    # targets = [x.replace(\" \", \"\") for x in batch[\"target\"]]\n",
    "    inputs = batch[\"input\"]\n",
    "    targets = batch[\"target\"]\n",
    "\n",
    "    # print(inputs, targets)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_separated = dataset_separated.map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "dataset_mixed = dataset_mixed.map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_separated_train = dataset_separated[\"train\"]\n",
    "dataset_separated_val = dataset_separated[\"val\"]\n",
    "dataset_separated_test = dataset_separated[\"test\"]\n",
    "\n",
    "dataset_mixed_train = dataset_mixed[\"train\"]\n",
    "dataset_mixed_val = dataset_mixed[\"val\"]\n",
    "dataset_mixed_test = dataset_mixed[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_train = concatenate_datasets([dataset_separated_train, dataset_mixed_train])\n",
    "combined_dataset_test = concatenate_datasets([dataset_separated_test, dataset_mixed_test])\n",
    "combined_dataset_val = concatenate_datasets([dataset_separated_val, dataset_mixed_val])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
