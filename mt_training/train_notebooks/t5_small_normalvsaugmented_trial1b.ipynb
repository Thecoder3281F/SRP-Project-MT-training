{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dad385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_normal = load_dataset(\"Thecoder3281f/MIT_separated_final\", \"normal\")\n",
    "dataset_augmented = load_dataset(\"Thecoder3281f/MIT_separated_final\", \"augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    inputs = batch[\"input\"]\n",
    "    targets = batch[\"target\"]\n",
    "\n",
    "    # print(inputs, targets)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normal = dataset_normal.map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])\n",
    "dataset_augmented = dataset_augmented.map(preprocess, batched=True, remove_columns=[\"input\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5350491",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_normal_train = dataset_normal[\"train\"]\n",
    "dataset_normal_val = dataset_normal[\"val\"]\n",
    "dataset_normal_test = dataset_normal[\"test\"]\n",
    "\n",
    "dataset_augmented_train = dataset_augmented[\"train\"]\n",
    "dataset_augmented_val = dataset_augmented[\"val\"]\n",
    "dataset_augmented_test = dataset_augmented[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def canonicalize(smiles):\n",
    "    \"\"\"Join tokens, parse to molecule, return canonical SMILES or None.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def tanimoto(a, b):\n",
    "    \"\"\"Compute Tanimoto similarity between two SMILES.\"\"\"\n",
    "    try:\n",
    "        ma, mb = Chem.MolFromSmiles(a), Chem.MolFromSmiles(b)\n",
    "        if not ma or not mb:\n",
    "            return 0\n",
    "        fa = AllChem.GetMorganFingerprintAsBitVect(ma, 2)\n",
    "        fb = AllChem.GetMorganFingerprintAsBitVect(mb, 2)\n",
    "        return DataStructs.TanimotoSimilarity(fa, fb)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # handle tuple\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # convert logits to token IDs if needed\n",
    "    preds = np.array(preds)\n",
    "    if preds.ndim == 3:  # (batch, seq_len, vocab_size)\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # handle top-1 outputs\n",
    "    if preds.ndim == 3:  # (batch, k, seq_len)\n",
    "        batch_size, k, seq_len = preds.shape\n",
    "        preds = preds.reshape(batch_size * k, seq_len)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_preds = [decoded_preds[i * k:(i + 1) * k] for i in range(batch_size)]\n",
    "    else:  # (batch, seq_len)\n",
    "        decoded_preds = [[tokenizer.decode(p, skip_special_tokens=True)] for p in preds]\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    top1_correct = 0\n",
    "    tanimotos = []\n",
    "    valids = []\n",
    "\n",
    "    for k_preds, label in zip(decoded_preds, decoded_labels):\n",
    "        label_c = canonicalize(label)\n",
    "        \n",
    "        if label_c is None:\n",
    "            tanimotos.append(0)\n",
    "            valids.append(0)\n",
    "            continue\n",
    "        valids.append(bool(Chem.MolFromSmiles(label_c)))\n",
    "        best_tani = 0\n",
    "\n",
    "        for i, pred in enumerate(k_preds):\n",
    "            p_c = canonicalize(pred)\n",
    "            if p_c is None:\n",
    "                continue\n",
    "\n",
    "            tani = tanimoto(p_c, label_c)\n",
    "            best_tani = max(best_tani, tani)\n",
    "\n",
    "            if p_c == label_c:\n",
    "                if i == 0:\n",
    "                    top1_correct += 1\n",
    "                break\n",
    "\n",
    "        tanimotos.append(best_tani)\n",
    "\n",
    "    canonical_top1 = top1_correct / len(decoded_labels)\n",
    "    mean_tanimoto = sum(tanimotos) / len(tanimotos)\n",
    "    validity = sum(valids) / len(valids)\n",
    "\n",
    "    logger.info(f\"Canonical Top-1 Accuracy: {canonical_top1:.3f}\")\n",
    "    logger.info(f\"Mean Tanimoto: {mean_tanimoto:.3f}\")\n",
    "    logger.info(f\"Validity: {validity:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"canonical_top1\": canonical_top1,\n",
    "        \"mean_tanimoto\": mean_tanimoto,\n",
    "        \"validity\": validity,\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add early stopping callback\n",
    "early_stop_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,   # stop if no improvement for 3 evals\n",
    "    early_stopping_threshold=0.001 # minimum change to qualify as improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir1 = \"t5-small-separated-40k\"\n",
    "output_dir2 = \"t5-small-augmented-20k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args1 = TrainingArguments(\n",
    "    output_dir=output_dir1,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    # per_device_train_batch_size=64,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=40000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    # num_train_epochs=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"./logs/{output_dir1}\",\n",
    "    run_name=output_dir1,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"canonical_top1\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer1 = Trainer(\n",
    "    model=model,\n",
    "    args=args1,\n",
    "    train_dataset=dataset_normal_train,\n",
    "    eval_dataset=dataset_normal_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[early_stop_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb33bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the final model\n",
    "# trainer1.save_model(f\"{output_dir1}\" + \"20000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = trainer1.evaluate(eval_dataset=dataset_normal_test)\n",
    "# print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args2 = TrainingArguments(\n",
    "    output_dir=output_dir2,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    # per_device_train_batch_size=64,\n",
    "    auto_find_batch_size=True,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # warmup_ratio=0.1,\n",
    "    max_steps=20000,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    # num_train_epochs=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    report_to=\"tensorboard\",\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"./logs/{output_dir2}\",\n",
    "    run_name=output_dir2,\n",
    "    greater_is_better=True,\n",
    "    metric_for_best_model=\"canonical_top1\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True,\n",
    "    eval_accumulation_steps=128,\n",
    "    fp16=True,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=args2,\n",
    "    train_dataset=dataset_augmented_train,\n",
    "    eval_dataset=dataset_augmented_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[early_stop_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad142eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer2.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da545f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "trainer2.save_model(f\"{output_dir2}\" + \"40000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srp-mt-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
