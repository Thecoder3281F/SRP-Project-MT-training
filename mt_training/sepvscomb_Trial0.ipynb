{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dad385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_separated = load_dataset(\"Thecoder3281f/MIT_separated\", \"normal\")\n",
    "dataset_mixed = load_dataset(\"Thecoder3281f/MIT_mixed\", \"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da07241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a037e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build vocab from your space-tokenized SMILES\n",
    "# def build_vocab_from_dataset(dataset, fields=[\"input\", \"target\"]):\n",
    "#     vocab = set()\n",
    "#     splits = [\"train\", \"val\", \"test\"]\n",
    "#     for split in splits:\n",
    "#         for ex in dataset[split]:\n",
    "#             for f in fields:\n",
    "#                 vocab.update(ex[f].split())\n",
    "\n",
    "#     return vocab\n",
    "\n",
    "# vocab = build_vocab_from_dataset(dataset)\n",
    "# vocab.update([\"[PAD]\", \"[UNK]\", \"<s>\", \"</s>\"])\n",
    "\n",
    "# #print(vocab)\n",
    "# vocab = {tok: i for i, tok in enumerate(sorted(vocab), start=0)}\n",
    "\n",
    "# #print(vocab)\n",
    "\n",
    "# # Create WordLevel tokenizer\n",
    "# tok = Tokenizer(WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "# tok.pre_tokenizer = Whitespace()\n",
    "\n",
    "# # Wrap as a Hugging Face tokenizer\n",
    "# hf_tokenizer = PreTrainedTokenizerFast(\n",
    "#     tokenizer_object=tok,\n",
    "#     unk_token=\"[UNK]\",\n",
    "#     pad_token=\"[PAD]\",\n",
    "#     eos_token=\"</s>\",\n",
    "#     bos_token=\"<s>\",\n",
    "# )\n",
    "\n",
    "# hf_tokenizer.save_pretrained(\"smiles-whitespace-tokenizer-separated-mit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_separated = PreTrainedTokenizerFast.from_pretrained(\"smiles-whitespace-tokenizer-separated-mit\")\n",
    "tokenizer_mixed = PreTrainedTokenizerFast.from_pretrained(\"smiles-whitespace-tokenizer-mixed-mit\")\n",
    "\n",
    "def preprocess(batch, tokenizer):\n",
    "    model_inputs = tokenizer(\n",
    "        batch[\"input\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            batch[\"target\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets_separated = dataset_separated.map(lambda batch: preprocess(batch, tokenizer_separated), batched=True, remove_columns=[\"input\", \"target\"])\n",
    "tokenized_datasets_mixed = dataset_mixed.map(lambda batch: preprocess(batch, tokenizer_mixed), batched=True, remove_columns=[\"input\", \"target\"])\n",
    "\n",
    "tokenized_datasets_separated, tokenized_datasets_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46fab0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e4c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# set up logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def canonicalize(smiles):\n",
    "    \"\"\"Join tokens, parse to molecule, return canonical SMILES or None.\"\"\"\n",
    "    try:\n",
    "        s = smiles.replace(\" \", \"\")\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        return Chem.MolToSmiles(mol, canonical=True)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def tanimoto(a, b):\n",
    "    \"\"\"Compute Tanimoto similarity between two SMILES.\"\"\"\n",
    "    try:\n",
    "        ma, mb = Chem.MolFromSmiles(a), Chem.MolFromSmiles(b)\n",
    "        if not ma or not mb:\n",
    "            return 0\n",
    "        fa = AllChem.GetMorganFingerprintAsBitVect(ma, 2)\n",
    "        fb = AllChem.GetMorganFingerprintAsBitVect(mb, 2)\n",
    "        return DataStructs.TanimotoSimilarity(fa, fb)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # handle tuple\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # convert logits to token IDs if needed\n",
    "    preds = np.array(preds)\n",
    "    if preds.ndim == 3:  # (batch, seq_len, vocab_size)\n",
    "        preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "\n",
    "    # ðŸ§  handle both top-1 and top-k outputs\n",
    "    if preds.ndim == 3:  # (batch, k, seq_len)\n",
    "        batch_size, k, seq_len = preds.shape\n",
    "        preds = preds.reshape(batch_size * k, seq_len)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_preds = [decoded_preds[i * k:(i + 1) * k] for i in range(batch_size)]\n",
    "    else:  # (batch, seq_len)\n",
    "        decoded_preds = [[tokenizer.decode(p, skip_special_tokens=True)] for p in preds]\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    top1_correct = 0\n",
    "    tanimotos = []\n",
    "    valids = []\n",
    "\n",
    "    for k_preds, label in zip(decoded_preds, decoded_labels):\n",
    "        label_c = canonicalize(label)\n",
    "        \n",
    "\n",
    "        if label_c is None:\n",
    "            tanimotos.append(0)\n",
    "            valids.append(0)\n",
    "            continue\n",
    "        valids.append(bool(Chem.MolFromSmiles(label_c)))\n",
    "        best_tani = 0\n",
    "\n",
    "        for i, pred in enumerate(k_preds):\n",
    "            p_c = canonicalize(pred)\n",
    "            if p_c is None:\n",
    "                continue\n",
    "\n",
    "            tani = tanimoto(p_c, label_c)\n",
    "            best_tani = max(best_tani, tani)\n",
    "\n",
    "            if p_c == label_c:\n",
    "                if i == 0:\n",
    "                    top1_correct += 1\n",
    "                break\n",
    "\n",
    "        tanimotos.append(best_tani)\n",
    "\n",
    "    canonical_top1 = top1_correct / len(decoded_labels)\n",
    "    mean_tanimoto = sum(tanimotos) / len(tanimotos)\n",
    "    validity = sum(valids) / len(valids)\n",
    "\n",
    "    logger.info(f\"Canonical Top-1 Accuracy: {canonical_top1:.3f}\")\n",
    "    logger.info(f\"Mean Tanimoto: {mean_tanimoto:.3f}\")\n",
    "    logger.info(f\"Validity: {validity:.3f}\")\n",
    "\n",
    "    return {\n",
    "        \"canonical_top1\": canonical_top1,\n",
    "        \"mean_tanimoto\": mean_tanimoto,\n",
    "        \"validity\": validity,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8c533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_small = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(40000))\n",
    "# val_dataset_small = tokenized_datasets[\"val\"].shuffle(seed=42).select(range(3000))\n",
    "# test_dataset_small = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(3000))\n",
    "\n",
    "train_dataset_separated = tokenized_datasets_separated[\"train\"]\n",
    "val_dataset_separated = tokenized_datasets_separated[\"val\"]\n",
    "test_dataset_separated = tokenized_datasets_separated[\"test\"]\n",
    "\n",
    "train_dataset_mixed = tokenized_datasets_mixed[\"train\"]\n",
    "val_dataset_mixed = tokenized_datasets_mixed[\"val\"]\n",
    "test_dataset_mixed = tokenized_datasets_mixed[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dc1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, T5ForConditionalGeneration, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(tokenizer):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1), labels\n",
    "\n",
    "\n",
    "def train_loop(sep_or_comb, model, tokenizer, train_dataset, val_dataset, num_steps=20000):\n",
    "    if sep_or_comb == \"separated\":\n",
    "        output_dir = f\"prelim-t5-small-mit-sepvscomb-separated\"\n",
    "    else:\n",
    "        output_dir = f\"prelim-t5-small-mit-sepvscomb-mixed\"\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        learning_rate=3e-4,\n",
    "        # per_device_train_batch_size=64,\n",
    "        auto_find_batch_size=True,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_ratio=0.1,\n",
    "        max_steps=num_steps,\n",
    "        save_steps=1000,\n",
    "        eval_steps=1000,\n",
    "        # num_train_epochs=1,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=100,\n",
    "        report_to=\"tensorboard\",\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        greater_is_better=True,\n",
    "        metric_for_best_model=\"canonical_top1\",\n",
    "        load_best_model_at_end=True,\n",
    "        gradient_checkpointing=True,\n",
    "        eval_accumulation_steps=128,\n",
    "        fp16=True,\n",
    "        save_total_limit=3,\n",
    "    )\n",
    "\n",
    "    # Add early stopping callback\n",
    "    early_stop_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=3,   # stop if no improvement for 3 evals\n",
    "        early_stopping_threshold=0.01 # minimum change to qualify as improvement\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        callbacks=[early_stop_callback],\n",
    "    )\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=False)\n",
    "\n",
    "    # Save the final model\n",
    "    trainer.save_model(f\"{output_dir}\" + str(num_steps))\n",
    "\n",
    "\n",
    "def test_loop(sep_or_comb, tokenizer, test_dataset):\n",
    "    if sep_or_comb == \"separated\":\n",
    "        output_dir = f\"prelim-t5-small-mit-sepvscomb-separated\"\n",
    "    else:\n",
    "        output_dir = f\"prelim-t5-small-mit-sepvscomb-mixed\"\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=T5ForConditionalGeneration.from_pretrained(output_dir),\n",
    "        args=args,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "    print(metrics)\n",
    "\n",
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     direction=\"minimize\",\n",
    "#     backend=\"optuna\",\n",
    "#     n_trials=5,\n",
    "#     hp_space=lambda trial: {\n",
    "#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-4),\n",
    "#         \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_separated = model_init(tokenizer_separated)\n",
    "model_mixed = model_init(tokenizer_mixed)\n",
    "\n",
    "model_separated, model_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5f3a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(\"separated\", model=model_separated, tokenizer=tokenizer_separated, train_dataset=train_dataset_separated, val_dataset=val_dataset_separated, num_steps=20000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d2eba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(\"mixed\", model=model_mixed, tokenizer=tokenizer_mixed, train_dataset=train_dataset_mixed, val_dataset=val_dataset_mixed, num_steps=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46361c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(\"separated\", tokenizer=tokenizer_separated, test_dataset=test_dataset_separated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f92181",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(\"mixed\", tokenizer=tokenizer_mixed, test_dataset=test_dataset_mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "def inference_4bit(tokenizer, text, model, num_return_sequences=2, num_beams=10):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=\"float16\",\n",
    "    )\n",
    "\n",
    "    model_4bit = T5ForConditionalGeneration.from_pretrained(\n",
    "        model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_token_type_ids=False).to(model_4bit.device)\n",
    "    outputs = model_4bit.generate(\n",
    "        **inputs, \n",
    "        max_length=256, \n",
    "        repetition_penalty=1.0, \n",
    "        do_sample=False, \n",
    "        num_return_sequences=num_return_sequences, \n",
    "        num_beams=num_beams, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    seq_scores = outputs.sequences_scores  # logits for each sequence\n",
    "    probs = torch.softmax(seq_scores, dim=0) * 100  # %\n",
    "    print(\"Scores: \", probs)\n",
    "\n",
    "    preds_4bit = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    for i, p in enumerate(preds_4bit):\n",
    "        print(f\"4-bit Model Prediction {i+1}: {p}\")\n",
    "\n",
    "def inference_8bit(tokenizer, text, model, num_return_sequences=2, num_beams=10):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "    )\n",
    "\n",
    "    model_8bit = T5ForConditionalGeneration.from_pretrained(\n",
    "        model,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_token_type_ids=False).to(model_8bit.device)\n",
    "    outputs = model_8bit.generate(\n",
    "        **inputs, \n",
    "        max_length=256, \n",
    "        repetition_penalty=1.0, \n",
    "        do_sample=False, \n",
    "        num_return_sequences=num_return_sequences, \n",
    "        num_beams=num_beams, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    seq_scores = outputs.sequences_scores  # logits for each sequence\n",
    "    probs = torch.softmax(seq_scores, dim=0) * 100  # %\n",
    "    print(\"Scores: \", probs)\n",
    "\n",
    "    preds_8bit = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    for i, p in enumerate(preds_8bit):\n",
    "        print(f\"8-bit Model Prediction {i+1}: {p}\")\n",
    "\n",
    "def inference_fp16(tokenizer, text, model, num_return_sequences=2, num_beams=10):\n",
    "    model_fp16 = T5ForConditionalGeneration.from_pretrained(\n",
    "        model,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_token_type_ids=False).to(model_fp16.device)\n",
    "    outputs = model_fp16.generate(\n",
    "        **inputs, \n",
    "        max_length=256, \n",
    "        repetition_penalty=1.0, \n",
    "        do_sample=False, \n",
    "        num_return_sequences=num_return_sequences, \n",
    "        num_beams=num_beams, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    seq_scores = outputs.sequences_scores  # logits for each sequence\n",
    "    probs = torch.softmax(seq_scores, dim=0) * 100  # %\n",
    "    print(\"Scores: \", probs)\n",
    "\n",
    "    preds_fp16 = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    for i, p in enumerate(preds_fp16):\n",
    "        print(f\"FP16 Model Prediction {i+1}: {p}\")\n",
    "\n",
    "def inference_fp32(tokenizer, text, model, num_return_sequences=2, num_beams=10):\n",
    "    model_fp32 = T5ForConditionalGeneration.from_pretrained(\n",
    "        model,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=\"auto\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", return_token_type_ids=False).to(model_fp32.device)\n",
    "    outputs = model_fp32.generate(\n",
    "        **inputs, \n",
    "        max_length=256, \n",
    "        repetition_penalty=1.0, \n",
    "        do_sample=False, \n",
    "        num_return_sequences=num_return_sequences, \n",
    "        num_beams=num_beams, \n",
    "        output_scores=True, \n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "\n",
    "    seq_scores = outputs.sequences_scores  # logits for each sequence\n",
    "    probs = torch.softmax(seq_scores, dim=0) * 100  # %\n",
    "    print(\"Scores: \", probs)\n",
    "\n",
    "    preds_fp32 = tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "    for i, p in enumerate(preds_fp32):\n",
    "        print(f\"FP32 Model Prediction {i+1}: {p}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2919b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"C O c 1 c c c c 2 c 1 C ( C ) C ( = O ) N 2 C . N # C C I > C C O . C C [O-] . [Na+]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7c4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inference_4bit(tokenizer_separated, text, \"t5-small-mit-sepvscomb-separated20000\", num_return_sequences=3, num_beams=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_8bit(tokenizer_separated, text, \"t5-small-mit-sepvscomb-separated20000\", num_return_sequences=3, num_beams=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e390d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_fp16(tokenizer_separated, text, \"t5-small-mit-sepvscomb-separated20000\", num_return_sequences=3, num_beams=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_fp32(tokenizer_separated, text, \"t5-small-mit-sepvscomb-separated20000\", num_return_sequences=3, num_beams=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
